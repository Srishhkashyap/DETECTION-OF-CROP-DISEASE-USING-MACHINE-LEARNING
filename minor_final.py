# -*- coding: utf-8 -*-
"""minor_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/166bMwhQZbalqxZM5yo1XsytRkw3JoSki
"""

pip install opendatasets

pip install pandas

import opendatasets as od
import pandas
  
od.download(
    "https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os


import keras

from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import img_to_array, load_img
from keras.applications.vgg19 import VGG19, preprocess_input, decode_predictions

#EDA(38 different leaf diseases)

len(os.listdir("/content/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train"))

#removed rescaling after writing preprocessing function 

train_datagen= ImageDataGenerator(zoom_range= 0.5, shear_range=0.3, horizontal_flip= True, preprocessing_function= preprocess_input )

val_datagen= ImageDataGenerator(preprocessing_function= preprocess_input)

train= train_datagen.flow_from_directory(directory="/content/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train", target_size= (256,256), batch_size=32)

val= val_datagen.flow_from_directory(directory="/content/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid")

#to figure out what kind of images are there
t_img, label= train.next()

def plotImage(img_arr, label):

  for im, l in zip(img_arr, label):
    plt.figure(figsize=(5,5))
    plt.imshow(im/180)
    plt.show
#if we change the number in plt.imshow the color of the preprocessing image changes

#some kind of preprocessing is done by use of some parameter given in the above function
plotImage(t_img[:3], label[:3])

"""**BUILDING OUR MODEL**"""

from keras.layers import Dense, Flatten
from keras.models import Model
from keras.applications.vgg19 import VGG19
import keras

base_model= VGG19(input_shape=(256,256,3), include_top= False)

for layer in base_model.layers:
  layer.trainable= False

base_model.summary()

X= Flatten()(base_model.output)

X= Dense(units= 38, activation= 'softmax')(X)

#Creating final model

model= Model(base_model.input, X)

model.summary()

model.compile(optimizer= 'adam', loss= keras.losses.categorical_crossentropy, metrics= ['accuracy'])

"""**EARLY STOPPING AND MODEL CHECKPOINT**"""

from keras.callbacks import ModelCheckpoint, EarlyStopping

#early stopping: monitors our validation
es= EarlyStopping(monitor= 'val_accuracy', min_delta= 0.01, patience= 3, verbose= 1)

#model checkpoint
mc= ModelCheckpoint(filepath= "best_model.h5",
                    monitor= 'val_accuracy',
                    min_delta= 0.01,
                    patience= 3,
                    verbose= 1,
                    save_best_only= True)

#adding this to an array bc when we train our model it takes it as array

cb= [es,mc]

his= model.fit_generator(train,
                         steps_per_epoch= 16,
                         epochs= 50,
                         verbose= 1,
                         callbacks= cb,
                         validation_data= val,
                         validation_steps= 16)

#our model got trained in different number of epochs everytime we run them, accuracy is fine.

h= his.history 
h.keys()

dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])

plt.plot(h['accuracy'])
plt.plot(h['val_accuracy'], c="red")
plt.title("acc vs v-acc")
plt.show()

plt.plot(h['loss'])
plt.plot(h['val_loss'], c="red")
plt.title("loss vs v-loss")
plt.show()

#load best model

from keras.models import load_model
model= load_model("/content/best_model.h5")

#to evaluate our model

acc= model.evaluate_generator(val)[1]

print(f"The accuracy of your model is= {acc*100}%")

ref= dict(zip(list(train.class_indices.values()), list(train.class_indices.keys())))

def prediction(path):

   img= load_img(path, target_size= (256,256))
   i= img_to_array(img)
   im= preprocess_input(i)
   img= np.expand_dims(im, axis=0)
   
   pred= np.argmax(model.predict(img))
   
   print(f"the image belongs to {ref[pred]}")

#we intially got an array, but if we change im to im.shape, we will get dimension. To make this work for our model, we are adding some parameter before im.shape

path= "/content/new-plant-diseases-dataset/test/test/TomatoYellowCurlVirus1.JPG"

prediction(path)

"""**STREAMLIT CODE**

**FOR DEPLOYMENT USING NGROK**
"""

from google.colab import drive
drive.mount('/content/drive')

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip

!unzip ngrok-stable-linux-amd64.zip

get_ipython().system_raw('./ngrok http 8501 &')

!curl -s http://localhost:4040/api/tunnels | python3 -c \
    'import sys, json; print("Execute the next cell and the go to the following URL: " +json.load(sys.stdin)["tunnels"][0]["public_url"])'

!pip install pyngrok

!ngrok authtoken 2I4yIUGveRKBuBhvGxid2KFF00r_64Saa4M9THgtbepuT9irf

pip install streamlit

!streamlit run /content/drive/MyDrive/minor_test2.py